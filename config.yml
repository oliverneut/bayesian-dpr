train:
  knowledge_distillation: True
  dataset_id: msmarco
  model_name: bert-base
  teacher_model_name: bert-base-msmarco
  max_qry_len: 32
  max_psg_len: 256
  alpha: 0.01
  k: 10
  prior_scale: 1.0
  wishart_scale: 0.1
  parameterization: diagonal
  batch_size: 16
  num_epochs: 20
  lr: 5.0e-6 # 5.0e-6
  min_lr: 5.0e-8 # 5.0e-8
  warmup_rate: 0.1

wandb:
  entity: oliver-neut-university-of-amsterdam
  project: bayesian-dpr
  run_id: avknlhh5

prepare_data:
  val_size: 100
  dataset_id: msmarco
  ce_score_margin: 3

eval:
  dataset_id: msmarco # msmarco or nq or hotpotqa or fiqa
  rel_mode: kl #dpr or kl
  embs_dir: output/models # /scratch-shared/tmp.alIhTYBxk7 or output/models