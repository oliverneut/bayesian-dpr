knowledge_distillation:
  dataset_id: msmarco
  model_name: bert-base
  num_samples: 10
  batch_size: 16
  num_epochs: 4
  lr: 5.0e-6
  min_lr: 5.0e-8
  warmup_rate: 0.1
  max_qry_len: 32
  max_psg_len: 256
  output_dir: output/distilled_models

prepare_data:
  num_samples: 502939
  val_ratio: 0.1