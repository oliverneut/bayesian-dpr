{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecd286dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "from vbll.layers.regression import VBLLReturn\n",
    "\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "base_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from src.utils.data_utils import DatasetConfig\n",
    "from src.data_loaders import get_queries, get_qrels\n",
    "from src.utils.model_utils import vbll_model_factory, model_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c82aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_queries(data: list, data_dir: Path, split: str) -> None:\n",
    "    \"\"\"Save queries to file.\"\"\"\n",
    "    with open(data_dir / f'queries-{split}.jsonl', 'wt', encoding='utf8') as f_out:\n",
    "        for query_data in tqdm(data, desc=f\"Saving {split} queries\"):\n",
    "            json.dump({\"query\": query_data[\"query\"], \"OOD\": query_data[\"OOD\"]}, f_out)\n",
    "            f_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c106f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_queries(test_queries: list, queries: Dict, data_cfg: DatasetConfig, num_samples: int, OOD: bool) -> None:\n",
    "    \"\"\"Prepare test queries dataset.\"\"\"\n",
    "    qrels = get_qrels(data_cfg.get_qrels_file(split=data_cfg.test_name))\n",
    "    \n",
    "    i = 0\n",
    "    for qid, rels in qrels.items():\n",
    "        if len(rels) > 0:\n",
    "            test_queries.append({\"query\": queries[qid], \"OOD\": OOD})\n",
    "            i += 1\n",
    "        \n",
    "        if i >= num_samples: break\n",
    "    \n",
    "    return test_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc6217fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco_cfg = DatasetConfig('msmarco')\n",
    "nq_cfg = DatasetConfig('nq')\n",
    "hotpotqa_cfg = DatasetConfig('hotpotqa')\n",
    "fiqa_cfg = DatasetConfig('fiqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f60f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco_queries = get_queries(msmarco_cfg.get_queries_file())\n",
    "nq_queries = get_queries(nq_cfg.get_queries_file())\n",
    "hotpotqa_queries = get_queries(hotpotqa_cfg.get_queries_file())\n",
    "fiqa_queries = get_queries(fiqa_cfg.get_queries_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2ba516",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = []\n",
    "test_queries = prepare_test_queries(test_queries, msmarco_queries, msmarco_cfg, 600, OOD=False)\n",
    "test_queries = prepare_test_queries(test_queries, nq_queries, nq_cfg, 200, OOD=True)\n",
    "test_queries = prepare_test_queries(test_queries, hotpotqa_queries, hotpotqa_cfg, 200, OOD=True)\n",
    "test_queries = prepare_test_queries(test_queries, fiqa_queries, fiqa_cfg, 200, OOD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b10afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving test queries: 100%|██████████| 1200/1200 [00:00<00:00, 157006.73it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(f'{base_path}/data/ood_detection')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "save_queries(test_queries, data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"10nfecme\"\n",
    "args = OmegaConf.load(f'{base_path}/config.yml')\n",
    "api = wandb.Api()\n",
    "config = api.run(f\"{args.wandb.entity}/{args.wandb.project}/{run_id}\").config\n",
    "params = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverneut/miniconda3/envs/bret/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /Users/oliverneut/Desktop/vbll-retrieval/output/models/10nfecme/model.pt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_dir = f\"{base_path}/output/models/{run_id}\"\n",
    "model_path = f\"{save_dir}/model.pt\"\n",
    "\n",
    "tokenizer, model = vbll_model_factory(params.model_name, 1, params.parameterization, params.prior_scale, params.wishart_scale, device)\n",
    "method = \"vbll\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "print(f'Loaded model from {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10d315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_query(qry: str, tokenizer, model):\n",
    "    qry_enc = tokenizer(qry, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "    qry_emb = model(qry_enc)\n",
    "    return qry_emb\n",
    "\n",
    "def uncertainty_score(scale, unc_method=\"norm\"):\n",
    "    cov = torch.diag(scale.squeeze())\n",
    "\n",
    "    if unc_method == \"norm\":\n",
    "        return torch.linalg.norm(cov)\n",
    "    elif unc_method == \"trace\":\n",
    "        return torch.trace(cov)\n",
    "    elif unc_method == \"det\":\n",
    "        _, logdet = torch.linalg.slogdet(cov)\n",
    "        return logdet\n",
    "    elif unc_method == \"entropy\":\n",
    "        d = cov.size(0)\n",
    "        _, logdet = torch.linalg.slogdet(cov)\n",
    "        return 0.5 * d * torch.log(torch.tensor(2 * torch.pi * torch.e)) + 0.5 * logdet\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown uncertainty method: {unc_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd741a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_uncertainty_scores(data, tokenizer, model, unc_method=\"norm\"):\n",
    "    uncertainty_scores = []\n",
    "    labels = []\n",
    "    for query_data in tqdm(data, desc=\"Calculating uncertainty scores\"):       \n",
    "        emb = infer_query(query_data['query'], tokenizer, model)\n",
    "\n",
    "        uncertainty_scores.append(uncertainty_score(emb.predictive.scale, unc_method).item())\n",
    "        labels.append(query_data['OOD'])\n",
    "\n",
    "    return np.array(uncertainty_scores), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08276572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msp_score(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return 1 - probs.max(dim=-1).values\n",
    "\n",
    "def entropy_score(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = torch.log(probs + 1e-12)\n",
    "    return -(probs * log_probs).sum(dim=-1)\n",
    "\n",
    "def energy_score(logits, temperature=1.0):\n",
    "    return -temperature * torch.logsumexp(logits / temperature, dim=-1)\n",
    "\n",
    "def calculate_baseline_scores(data, tokenizer, model):\n",
    "    msp_scores = []\n",
    "    entropy_scores = []\n",
    "    energy_scores = []\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for query_data in tqdm(data, desc=\"Calculating uncertainty scores\"):  \n",
    "        emb = infer_query(query_data['query'], tokenizer, model)\n",
    "        if isinstance(emb, VBLLReturn):\n",
    "            emb = emb.predictive.loc\n",
    "        msp_scores.append(msp_score(emb).item())\n",
    "        entropy_scores.append(entropy_score(emb).item())\n",
    "        energy_scores.append(energy_score(emb).item())\n",
    "        \n",
    "        labels.append(query_data['OOD'])\n",
    "\n",
    "    return np.array(msp_scores), np.array(entropy_scores), np.array(energy_scores), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85089685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(uncertainty_scores, labels):\n",
    "    auc = roc_auc_score(labels, uncertainty_scores)\n",
    "    print(f\"AUROC: {auc}\")\n",
    "    aupr = average_precision_score(labels, uncertainty_scores)\n",
    "    print(f\"AUPR: {aupr}\")\n",
    "    pbs = pointbiserialr(labels, uncertainty_scores)\n",
    "    print(f\"Point Biserial Correlation: {pbs.correlation}, p-value: {pbs.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c053610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 1200/1200 [00:26<00:00, 45.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty scores calculated using method norm\n",
      "AUROC: 0.6764111111111111\n",
      "AUPR: 0.716080567834626\n",
      "Point Biserial Correlation: 0.3416073211741685, p-value: 3.5199246850802196e-34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 1200/1200 [00:22<00:00, 54.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline scores calculated\n",
      "AUROC: 0.49325277777777776\n",
      "AUPR: 0.4846359401299988\n",
      "Point Biserial Correlation: 0.026524068711434954, p-value: 0.35860596784544824\n",
      "AUROC: 0.4908138888888889\n",
      "AUPR: 0.48365730012417474\n",
      "Point Biserial Correlation: 0.0005153832906049391, p-value: 0.9857706460820609\n",
      "AUROC: 0.4747152777777778\n",
      "AUPR: 0.46767619877594346\n",
      "Point Biserial Correlation: -0.05829954785093843, p-value: 0.043469719960737396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unc_method = \"norm\"\n",
    "uncertainty_scores, labels = calculate_uncertainty_scores(test_queries, tokenizer, model, unc_method)\n",
    "print(f\"Uncertainty scores calculated using method {unc_method}\")\n",
    "metrics(- 1 * uncertainty_scores, labels)\n",
    "print('')\n",
    "\n",
    "msp_scores, entropy_scores, energy_scores, labels = calculate_baseline_scores(test_queries, tokenizer, model)\n",
    "print(f\"Baseline scores calculated\")\n",
    "metrics(- 1 * msp_scores, labels)\n",
    "metrics(- 1 * entropy_scores, labels)\n",
    "metrics(- 1 * energy_scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe5e3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco_nq_queries = []\n",
    "msmarco_nq_queries = prepare_test_queries(msmarco_nq_queries, msmarco_queries, msmarco_cfg, 1000, OOD=False)\n",
    "msmarco_nq_queries = prepare_test_queries(msmarco_nq_queries, nq_queries, nq_cfg, 1000, OOD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a36182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 2000/2000 [00:46<00:00, 42.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty scores calculated using method norm\n",
      "AUROC: 0.5191865\n",
      "AUPR: 0.5183275376163669\n",
      "Point Biserial Correlation: 0.040910056466032164, p-value: 0.06737335253302827\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 2000/2000 [00:38<00:00, 51.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline scores calculated\n",
      "AUROC: 0.47432799999999997\n",
      "AUPR: 0.47777346269505894\n",
      "Point Biserial Correlation: -0.02809978149238754, p-value: 0.20907144432141286\n",
      "AUROC: 0.476193\n",
      "AUPR: 0.4812363557834576\n",
      "Point Biserial Correlation: -0.0407419799981015, p-value: 0.0685075050534339\n",
      "AUROC: 0.468404\n",
      "AUPR: 0.47599684086928695\n",
      "Point Biserial Correlation: -0.0602108317727549, p-value: 0.007071235899221918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uncertainty_scores, labels = calculate_uncertainty_scores(msmarco_nq_queries, tokenizer, model, unc_method=\"norm\")\n",
    "print(f\"Uncertainty scores calculated using method {unc_method}\")\n",
    "metrics(- 1 * uncertainty_scores, labels)\n",
    "print('')\n",
    "\n",
    "msp_scores, entropy_scores, energy_scores, labels = calculate_baseline_scores(msmarco_nq_queries, tokenizer, model)\n",
    "print(f\"Baseline scores calculated\")\n",
    "metrics(- 1 * msp_scores, labels)\n",
    "metrics(- 1 * entropy_scores, labels)\n",
    "metrics(- 1 * energy_scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baba39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco_hotpotqa_queries = []\n",
    "msmarco_hotpotqa_queries = prepare_test_queries(msmarco_hotpotqa_queries, msmarco_queries, msmarco_cfg, 1000, OOD=False)\n",
    "msmarco_hotpotqa_queries = prepare_test_queries(msmarco_hotpotqa_queries, hotpotqa_queries, hotpotqa_cfg, 1000, OOD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca8791a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 2000/2000 [00:44<00:00, 45.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty scores calculated using method norm\n",
      "AUROC: 0.914074\n",
      "AUPR: 0.9192207918480995\n",
      "Point Biserial Correlation: 0.7003532203072199, p-value: 7.041412097038473e-295\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 2000/2000 [00:44<00:00, 45.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline scores calculated\n",
      "AUROC: 0.471125\n",
      "AUPR: 0.4578007529639321\n",
      "Point Biserial Correlation: 0.017223619734340057, p-value: 0.44139526048974737\n",
      "AUROC: 0.46131099999999997\n",
      "AUPR: 0.4529283071357134\n",
      "Point Biserial Correlation: -0.0336054746041069, p-value: 0.13300228567433267\n",
      "AUROC: 0.3954805\n",
      "AUPR: 0.41480551694049533\n",
      "Point Biserial Correlation: -0.19973672571349443, p-value: 1.9167873718608717e-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uncertainty_scores, labels = calculate_uncertainty_scores(msmarco_hotpotqa_queries, tokenizer, model, unc_method=\"norm\")\n",
    "print(f\"Uncertainty scores calculated using method {unc_method}\")\n",
    "metrics(- 1 * uncertainty_scores, labels)\n",
    "print('')\n",
    "\n",
    "msp_scores, entropy_scores, energy_scores, labels = calculate_baseline_scores(msmarco_hotpotqa_queries, tokenizer, model)\n",
    "print(f\"Baseline scores calculated\")\n",
    "metrics(- 1 * msp_scores, labels)\n",
    "metrics(- 1 * entropy_scores, labels)\n",
    "metrics(- 1 * energy_scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d1a418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco_fiqa_queries = []\n",
    "msmarco_fiqa_queries = prepare_test_queries(msmarco_fiqa_queries, msmarco_queries, msmarco_cfg, 1000, OOD=True)\n",
    "msmarco_fiqa_queries = prepare_test_queries(msmarco_fiqa_queries, fiqa_queries, fiqa_cfg, 1000, OOD=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f01df336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores:   0%|          | 0/1500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'predictive'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m uncertainty_scores, labels = \u001b[43mcalculate_uncertainty_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsmarco_fiqa_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munc_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnorm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUncertainty scores calculated using method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munc_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m metrics(uncertainty_scores, labels)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcalculate_uncertainty_scores\u001b[39m\u001b[34m(data, tokenizer, model, unc_method)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query_data \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc=\u001b[33m\"\u001b[39m\u001b[33mCalculating uncertainty scores\u001b[39m\u001b[33m\"\u001b[39m):       \n\u001b[32m      5\u001b[39m     emb = infer_query(query_data[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m], tokenizer, model)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     uncertainty_scores.append(uncertainty_score(\u001b[43memb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredictive\u001b[49m.scale, unc_method).item())\n\u001b[32m      8\u001b[39m     labels.append(query_data[\u001b[33m'\u001b[39m\u001b[33mOOD\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(uncertainty_scores), np.array(labels)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'predictive'"
     ]
    }
   ],
   "source": [
    "uncertainty_scores, labels = calculate_uncertainty_scores(msmarco_fiqa_queries, tokenizer, model, unc_method=\"norm\")\n",
    "print(f\"Uncertainty scores calculated using method {unc_method}\")\n",
    "metrics(uncertainty_scores, labels)\n",
    "print('')\n",
    "\n",
    "msp_scores, entropy_scores, energy_scores, labels = calculate_baseline_scores(msmarco_fiqa_queries, tokenizer, model)\n",
    "print(f\"Baseline scores calculated\")\n",
    "metrics(- 1 * msp_scores, labels)\n",
    "metrics(- 1 * entropy_scores, labels)\n",
    "metrics(- 1 * energy_scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38d34a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"yi84sy0n\"\n",
    "args = OmegaConf.load(f'{base_path}/config.yml')\n",
    "api = wandb.Api()\n",
    "config = api.run(f\"{args.wandb.entity}/{args.wandb.project}/{run_id}\").config\n",
    "params = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11b29395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /Users/oliverneut/Desktop/vbll-retrieval/output/models/yi84sy0n/model.pt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "save_dir = f\"{base_path}/output/models/{run_id}\"\n",
    "model_path = f\"{save_dir}/model.pt\"\n",
    "\n",
    "tokenizer, model = model_factory(params.model_name, device)\n",
    "method = \"vbll\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "print(f'Loaded model from {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e705bac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 2000/2000 [00:35<00:00, 55.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline scores calculated\n",
      "AUROC: 0.6748365\n",
      "AUPR: 0.6575799336554933\n",
      "Point Biserial Correlation: 0.2912875530984401, p-value: 2.0724625916697796e-40\n",
      "AUROC: 0.7024264999999998\n",
      "AUPR: 0.6848219845137927\n",
      "Point Biserial Correlation: 0.3562519724269746, p-value: 6.510096012965339e-61\n",
      "AUROC: 0.691584\n",
      "AUPR: 0.6769874338783015\n",
      "Point Biserial Correlation: 0.3391698477462174, p-value: 4.990034077487274e-55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "msmarco_nq_queries = []\n",
    "msmarco_nq_queries = prepare_test_queries(msmarco_nq_queries, msmarco_queries, msmarco_cfg, 1000, OOD=False)\n",
    "msmarco_nq_queries = prepare_test_queries(msmarco_nq_queries, nq_queries, nq_cfg, 1000, OOD=True)\n",
    "\n",
    "msp_scores, entropy_scores, energy_scores, labels = calculate_baseline_scores(msmarco_nq_queries, tokenizer, model)\n",
    "print(f\"Baseline scores calculated\")\n",
    "metrics(- 1 * msp_scores, labels)\n",
    "metrics(- 1 * entropy_scores, labels)\n",
    "metrics(- 1 * energy_scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ade17695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 2000/2000 [00:36<00:00, 55.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline scores calculated\n",
      "AUROC: 0.825715\n",
      "AUPR: 0.8079676867322038\n",
      "Point Biserial Correlation: 0.5290336783813766, p-value: 1.1941099073968296e-144\n",
      "AUROC: 0.9441105000000001\n",
      "AUPR: 0.937589291894521\n",
      "Point Biserial Correlation: 0.7469927658654278, p-value: 0.0\n",
      "AUROC: 0.9372135\n",
      "AUPR: 0.9321890456304286\n",
      "Point Biserial Correlation: 0.7342374998637424, p-value: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "msmarco_hotpotqa_queries = []\n",
    "msmarco_hotpotqa_queries = prepare_test_queries(msmarco_hotpotqa_queries, msmarco_queries, msmarco_cfg, 1000, OOD=False)\n",
    "msmarco_hotpotqa_queries = prepare_test_queries(msmarco_hotpotqa_queries, hotpotqa_queries, hotpotqa_cfg, 1000, OOD=True)\n",
    "\n",
    "msp_scores, entropy_scores, energy_scores, labels = calculate_baseline_scores(msmarco_hotpotqa_queries, tokenizer, model)\n",
    "print(f\"Baseline scores calculated\")\n",
    "metrics(- 1 * msp_scores, labels)\n",
    "metrics(- 1 * entropy_scores, labels)\n",
    "metrics(- 1 * energy_scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b60c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating uncertainty scores: 100%|██████████| 1500/1500 [00:25<00:00, 57.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline scores calculated\n",
      "AUROC: 0.322836\n",
      "AUPR: 0.5600879526452014\n",
      "Point Biserial Correlation: -0.2861443828096443, p-value: 1.1655157957269113e-29\n",
      "AUROC: 0.187816\n",
      "AUPR: 0.49640957422483656\n",
      "Point Biserial Correlation: -0.5071649098866917, p-value: 7.657117787132468e-99\n",
      "AUROC: 0.20453\n",
      "AUPR: 0.5026598452551934\n",
      "Point Biserial Correlation: -0.48145016550341263, p-value: 7.133268741670028e-88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "msmarco_fiqa_queries = []\n",
    "msmarco_fiqa_queries = prepare_test_queries(msmarco_fiqa_queries, msmarco_queries, msmarco_cfg, 1000, OOD=True)\n",
    "msmarco_fiqa_queries = prepare_test_queries(msmarco_fiqa_queries, fiqa_queries, fiqa_cfg, 1000, OOD=False)\n",
    "\n",
    "msp_scores, entropy_scores, energy_scores, labels = calculate_baseline_scores(msmarco_fiqa_queries, tokenizer, model)\n",
    "print(f\"Baseline scores calculated\")\n",
    "metrics(- 1 * msp_scores, labels)\n",
    "metrics(- 1 * entropy_scores, labels)\n",
    "metrics(- 1 * energy_scores, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
